<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" hreflang="en" /><updated>2026-02-13T04:29:47+00:00</updated><id>/feed.xml</id><title type="html">Mehrdad Mohammadi</title><subtitle>University of Illinois at Urbana-Champaign</subtitle><author><name>Mehrdad Mohammadi</name></author><entry><title type="html">Implicit Supervised Learning; Reinforcement Learning</title><link href="/Implicit-Supervised-Learning/" rel="alternate" type="text/html" title="Implicit Supervised Learning; Reinforcement Learning" /><published>2023-08-05T00:00:00+00:00</published><updated>2023-08-05T00:00:00+00:00</updated><id>/Implicit-Supervised-Learning</id><content type="html" xml:base="/Implicit-Supervised-Learning/"><![CDATA[<p>How can (machine) agents learn from experience without a  teacher explicitly telling them what to do? Reinforcement learning is the area within (machine) learning that investigates how an agent can learn an optimal behavior by correlating a reward with its past actions. It draws from several disciplines, including behavioral psychology, economics, and operations research, to model the learning process. In this approach, the environment is typically modeled as a Markov decision process, where the agent receives immediate reward and state information but must learn how to choose actions that maximize its overall reward over time.\</p>

<p>Supervised learning is the most common data analysis and has been extensively studied in the statistics community for a long time. Supervised learning is a sound approach, but collecting input-output paired data is often too expensive. Unsupervised learning, on the other hand, is inexpensive to perform, but it tends to be ad hoc. Reinforcement learning is placed between supervised learning and unsupervised learning, but we still want to learn the input-output relation behind the data. Instead of output data, reinforcement learning utilizes rewards, which evaluate the validity of predicted outputs. Giving implicit supervision such as rewards is usually much easier and less costly than giving explicit supervision. Various supervised and unsupervised learning techniques are also utilized in the framework of reinforcement learning.\</p>

<p>To illustrate a reinforcement learning problem, consider a maze  where a robot must reach the goal without guidance on which direction to take. In this problem, states are positions in the maze, actions are possible directions, and transitions describe how states connect through actions. Rewards indicate the incomes or costs that the robot agent receives, and the goal is to find the policy that allows the robot agent to receive the maximum amount of rewards in the long run. To achieve this goal efficiently, dynamic programming can be used to obtain the (state-)value, which is the return as a function of the initial state. This method breaks down the complex optimization problem into simpler subproblems and solves them recursively, reusing solutions to reduce computation costs.\</p>

<p>Let $Q$ expected utility or value \citep{sutton2018reinforcement}, Bellman’s equation  describes this value in terms of the expected reward and the expected outcome of the random transition $(x,a)\mapsto (X’,A’)$:
\(Q(x,a)=\mathbb{E} R(x,a)+ \gamma \mathbb{E} Q(X',A')\)</p>

<p>Let $Z$ the random return whose expectation is the value $Q$. This random return is described by a recursive equation, but one of a distributional nature. The distributional Bellman equation \citep{bellemare2017distributional} states that the distribution of $Z$ is characterized by the interaction of three random variables: the reward $R$, the next state-action $(X’,A’)$, and
its random return $Z(X’,A’)$:
\(Z(x,a)\overset{D}{=} R(x,a)+ \gamma Z(X',A')\)
\cite{bellemare2017distributional} show that, for a fixed policy, the distributional Bellman operator is a contraction in a maximal form of the Wasserstein metric. However, the same operator is not a contraction in total variation, Kullback-Leibler divergence, or Kolmogorov distance.\</p>

<p>Unlike ordinary RL that the optimality version of Bellman equation has a unique fixed point $Q^\ast$,the optimal value function, \citep{sutton2018reinforcement} corresponding to the set of optimal policies, an optimality analog of Distributional Bellman Equation does not have a unique fixed point. In fact, the distributional operator defined over return is not a contraction \citep{bellemare2017distributional}.\</p>

<p>Existing distributional RL algorithms parameterize the policy return distribution in many different ways, including canonical return atoms \citep{bellemare2017distributional} , the expectiles \citep{rowland2019statistics}, the moments \citep{nguyen2021distributional}, and the quantiles \citep{dabney2018distributional,  yang2019fully, dabney2018implicit}.\</p>]]></content><author><name>Mehrdad Mohammadi</name></author><category term="Learning Theory" /><category term="Learning_Theory" /><category term="Overview" /><summary type="html"><![CDATA[How can (machine) agents learn from experience without a teacher explicitly telling them what to do? Reinforcement learning is the area within (machine) learning that investigates how an agent can learn an optimal behavior by correlating a reward with its past actions. It draws from several disciplines, including behavioral psychology, economics, and operations research, to model the learning process. In this approach, the environment is typically modeled as a Markov decision process, where the agent receives immediate reward and state information but must learn how to choose actions that maximize its overall reward over time.\ Supervised learning is the most common data analysis and has been extensively studied in the statistics community for a long time. Supervised learning is a sound approach, but collecting input-output paired data is often too expensive. Unsupervised learning, on the other hand, is inexpensive to perform, but it tends to be ad hoc. Reinforcement learning is placed between supervised learning and unsupervised learning, but we still want to learn the input-output relation behind the data. Instead of output data, reinforcement learning utilizes rewards, which evaluate the validity of predicted outputs. Giving implicit supervision such as rewards is usually much easier and less costly than giving explicit supervision. Various supervised and unsupervised learning techniques are also utilized in the framework of reinforcement learning.\ To illustrate a reinforcement learning problem, consider a maze where a robot must reach the goal without guidance on which direction to take. In this problem, states are positions in the maze, actions are possible directions, and transitions describe how states connect through actions. Rewards indicate the incomes or costs that the robot agent receives, and the goal is to find the policy that allows the robot agent to receive the maximum amount of rewards in the long run. To achieve this goal efficiently, dynamic programming can be used to obtain the (state-)value, which is the return as a function of the initial state. This method breaks down the complex optimization problem into simpler subproblems and solves them recursively, reusing solutions to reduce computation costs.\ Let $Q$ expected utility or value \citep{sutton2018reinforcement}, Bellman’s equation describes this value in terms of the expected reward and the expected outcome of the random transition $(x,a)\mapsto (X’,A’)$: \(Q(x,a)=\mathbb{E} R(x,a)+ \gamma \mathbb{E} Q(X',A')\) Let $Z$ the random return whose expectation is the value $Q$. This random return is described by a recursive equation, but one of a distributional nature. The distributional Bellman equation \citep{bellemare2017distributional} states that the distribution of $Z$ is characterized by the interaction of three random variables: the reward $R$, the next state-action $(X’,A’)$, and its random return $Z(X’,A’)$: \(Z(x,a)\overset{D}{=} R(x,a)+ \gamma Z(X',A')\) \cite{bellemare2017distributional} show that, for a fixed policy, the distributional Bellman operator is a contraction in a maximal form of the Wasserstein metric. However, the same operator is not a contraction in total variation, Kullback-Leibler divergence, or Kolmogorov distance.\ Unlike ordinary RL that the optimality version of Bellman equation has a unique fixed point $Q^\ast$,the optimal value function, \citep{sutton2018reinforcement} corresponding to the set of optimal policies, an optimality analog of Distributional Bellman Equation does not have a unique fixed point. In fact, the distributional operator defined over return is not a contraction \citep{bellemare2017distributional}.\ Existing distributional RL algorithms parameterize the policy return distribution in many different ways, including canonical return atoms \citep{bellemare2017distributional} , the expectiles \citep{rowland2019statistics}, the moments \citep{nguyen2021distributional}, and the quantiles \citep{dabney2018distributional, yang2019fully, dabney2018implicit}.\]]></summary></entry><entry><title type="html">Information Theory Applications Glimpse in Statistics</title><link href="/Information-Theory-Applications-Glimpse-in-Statistics/" rel="alternate" type="text/html" title="Information Theory Applications Glimpse in Statistics" /><published>2021-11-12T00:00:00+00:00</published><updated>2021-11-12T00:00:00+00:00</updated><id>/Information-Theory-Applications-Glimpse-in-Statistics</id><content type="html" xml:base="/Information-Theory-Applications-Glimpse-in-Statistics/"><![CDATA[<p><strong>Era of massive data sets brings fascinating problems at the interfaces between information theory and
(statistical) learning theory.</strong></p>

<p><br />
There are various studied connections between information theory and statistics:</p>
<ul>
  <li>Hypothesis testing, large deviations</li>
  <li>Fisher information, Kullback-Leibler divergence</li>
  <li>Metric entropy and Fano’s inequality</li>
  <li> etc...</li>
</ul>
<p><br />
 In their classic paper, Kolmogorov and Tikhomirov(1959) make connections between statistical estimation, metric entropy and the notion of channel capacity. Let’s write and draw this in information theoretic jargon. Let; <br /></p>

<p><strong>Codebook:</strong> indexed parametric family of probability distributions \(\{Q_\theta | \theta \in
\Theta\}\) <br />
 <strong>Codeword:</strong> nature chooses some parameter \(\theta^\ast \in
\Theta\) (the so-called True Parameter value) <br />
 <strong>Channel:</strong> user observes \(n\) i.i.d. draws from the true distribution \(X_i \sim Q_\theta^\ast\)<br />
 <strong>Decoding:</strong> estimator \(X^n \mapsto \hat{\theta}\) such that \(\hat{\theta}\overset{\mathbb{P}}{\rightarrow}\theta^\ast\) <br />
 <br />
 The setting could be defined in many variations:</p>

<table style="border-collapse:collapse;border-spacing:0;border:none" class="tg"><tbody><tr><td style="border-style:solid;border-width:0px;font-family:Arial, Helvetica, sans-serif !important;;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">codebooks/codewords:</td><td style="border-style:solid;border-width:0px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">graphs, vectors, matrices, functions, densities,etc.</td></tr><tr><td style="border-style:solid;border-width:0px;font-family:Arial, Helvetica, sans-serif !important;;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">channels:</td><td style="border-style:solid;border-width:0px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">random graphs, regression models, elementwise probes of vectors/machines, random projections ,etc.</td></tr><tr><td style="border-style:solid;border-width:0px;font-family:Arial, Helvetica, sans-serif !important;;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">closeness notion:</td><td style="border-style:solid;border-width:0px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">exact/partial graph recovery in Hamming, \(l^p\) distances, \(L^P\)(Q)-distances, sup-norm etc.</td></tr></tbody></table>

<p><img src="/images/blog/pic1.png" align="center" style="height:250px" /></p>]]></content><author><name>Mehrdad Mohammadi</name></author><category term="Information Theoey and Learning Theory" /><summary type="html"><![CDATA[Era of massive data sets brings fascinating problems at the interfaces between information theory and (statistical) learning theory. There are various studied connections between information theory and statistics: Hypothesis testing, large deviations Fisher information, Kullback-Leibler divergence Metric entropy and Fano’s inequality etc... In their classic paper, Kolmogorov and Tikhomirov(1959) make connections between statistical estimation, metric entropy and the notion of channel capacity. Let’s write and draw this in information theoretic jargon. Let; Codebook: indexed parametric family of probability distributions \(\{Q_\theta | \theta \in \Theta\}\) Codeword: nature chooses some parameter \(\theta^\ast \in \Theta\) (the so-called True Parameter value) Channel: user observes \(n\) i.i.d. draws from the true distribution \(X_i \sim Q_\theta^\ast\) Decoding: estimator \(X^n \mapsto \hat{\theta}\) such that \(\hat{\theta}\overset{\mathbb{P}}{\rightarrow}\theta^\ast\) The setting could be defined in many variations:]]></summary></entry></feed>